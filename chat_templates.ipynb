{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Chat Templates with SmolLM2\n",
    " Chat templates help structure interactions between users and AI models, ensuring consistent and contextually appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets trl huggingface_hub python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env文件\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import setup_chat_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM2 Chat Template\n",
    "定义一个简单的对话并应用聊天模板。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically set the device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "cache_dir = \"../../huggingface\"\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name, cache_dir=cache_dir).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, cache_dir=cache_dir)\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define messages for SmolLM2\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm doing well, thank you! How can I assist you today?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply chat template without tokenization\n",
    "分词器将对话表示为带有特殊标记的字符串，用于描述用户和助手的角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation with template: <|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm doing well, thank you! How can I assist you today?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "print(\"Conversation with template:\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation decoded: <|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm doing well, thank you! How can I assist you today?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Conversation decoded:\", tokenizer.decode(token_ids=input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the conversation\n",
    "分词器还会将对话和特殊分词作为与模型词汇相关的id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation tokenized: [1, 4093, 198, 19556, 28, 638, 359, 346, 47, 2, 198, 1, 520, 9531, 198, 57, 5248, 2567, 876, 28, 9984, 346, 17, 1073, 416, 339, 4237, 346, 1834, 47, 2, 198, 1, 520, 9531, 198]\n"
     ]
    }
   ],
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "print(\"Conversation tokenized:\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Process a dataset for SFT\n",
    "从HuggingFace获取数据集，并针对 SFT 对其进行处理。\n",
    "\n",
    "🐢 Convert the `HuggingFaceTB/smoltalk` dataset into chatml format.\n",
    "\n",
    "🐕 Convert the `openai/gsm8k` dataset into chatml format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_q/jbpkx1dj3r17p3j18038ldhw0000gn/T/ipykernel_60953/4068687702.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "  src=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0\"\n",
       "  frameborder=\"0\"\n",
       "  width=\"100%\"\n",
       "  height=\"360px\"\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        \"\"\"<iframe\n",
    "  src=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"360px\"\n",
    "></iframe>\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 2260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 119\n",
      "    })\n",
      "})\n",
      "----------------------------\n",
      "{'full_topic': 'Travel/Vacation destinations/Beach resorts', 'messages': [{'content': 'Hi there', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\", 'role': 'user'}, {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\", 'role': 'assistant'}, {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?', 'role': 'user'}, {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.', 'role': 'assistant'}, {'content': \"Okay, I'll look into those. Thanks for the recommendations!\", 'role': 'user'}, {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\", 'role': 'assistant'}]}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 2260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 119\n",
      "    })\n",
      "})\n",
      "----------------------------\n",
      "{'full_topic': 'Travel/Vacation destinations/Beach resorts', 'messages': [1, 4093, 198, 26843, 665, 2, 198, 1, 520, 9531, 198, 19556, 17, 1073, 416, 339, 724, 346, 1834, 47, 2, 198, 1, 4093, 198, 57, 5248, 3012, 327, 253, 10724, 14500, 327, 957, 1867, 17584, 30, 1978, 346, 3730, 634, 2378, 2911, 47, 2, 198, 1, 520, 9531, 198, 4449, 2378, 10724, 36088, 1453, 48326, 281, 14126, 28, 260, 48148, 898, 28, 284, 260, 44057, 30, 1069, 2316, 1343, 327, 480, 3953, 16351, 284, 13253, 29, 10086, 5656, 30, 2, 198, 1, 4093, 198, 5195, 4598, 1109, 30, 4184, 665, 750, 36088, 281, 260, 11981, 338, 359, 1123, 327, 3168, 47, 2, 198, 1, 520, 9531, 198, 10539, 28, 260, 25518, 284, 7784, 48096, 10015, 284, 47557, 395, 359, 5412, 4975, 327, 1564, 29, 9263, 36088, 281, 260, 11981, 30, 1069, 2626, 253, 1845, 282, 2123, 284, 32255, 5712, 327, 511, 6399, 30, 2, 198, 1, 4093, 198, 39122, 28, 339, 3060, 1492, 618, 967, 30, 10090, 327, 260, 7400, 17, 2, 198, 1, 520, 9531, 198, 2683, 2316, 10668, 30, 339, 3826, 346, 1042, 260, 3468, 14500, 327, 469, 17584, 30, 2, 198, 1, 520, 9531, 198]}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(\"----------------------------\")\n",
    "print(ds['train'][0])\n",
    "\n",
    "def process_dataset(sample):\n",
    "    # TODO: 🐢 Convert the sample into a chat format\n",
    "    # use the tokenizer's method to apply the chat template\n",
    "    sample['messages'] = tokenizer.apply_chat_template(sample['messages'],add_generation_prompt=True, tokenize=True)\n",
    "    return sample\n",
    "\n",
    "ds_processed = ds.map(process_dataset)\n",
    "print(ds_processed)\n",
    "print(\"----------------------------\")\n",
    "print(ds_processed['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nHi there<|im_end|>\\n<|im_start|>assistant\\nHello! How can I help you today?<|im_end|>\\n<|im_start|>user\\nI'm looking for a beach resort for my next vacation. Can you recommend some popular ones?<|im_end|>\\n<|im_start|>assistant\\nSome popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.<|im_end|>\\n<|im_start|>user\\nThat sounds great. Are there any resorts in the Caribbean that are good for families?<|im_end|>\\n<|im_start|>assistant\\nYes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.<|im_end|>\\n<|im_start|>user\\nOkay, I'll look into those. Thanks for the recommendations!<|im_end|>\\n<|im_start|>assistant\\nYou're welcome. I hope you find the perfect resort for your vacation.<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ds_processed['train'][0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "  src=\"https://huggingface.co/datasets/openai/gsm8k/embed/viewer/main/train\"\n",
       "  frameborder=\"0\"\n",
       "  width=\"100%\"\n",
       "  height=\"360px\"\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        \"\"\"<iframe\n",
    "  src=\"https://huggingface.co/datasets/openai/gsm8k/embed/viewer/main/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"360px\"\n",
    "></iframe>\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d865e5a58d63413f8df550fc5146bc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e85ffa8b1245f1a9cb68d74e8cf82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8c20377bd84a7883238ad1fb8924ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275b1895189a445ab1275b742728a1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887e62899ec84012b1ca44c25e63cbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?',\n",
       " 'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'messages'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'messages'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "})\n",
      "----------------------------\n",
      "{'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10', 'messages': [1, 4093, 198, 71, 1059, 38668, 1885, 33, 34, 354, 5353, 327, 3383, 672, 9584, 30, 718, 15955, 28, 1041, 915, 1250, 216, 37, 32, 3487, 282, 3383, 672, 9584, 30, 1073, 1083, 1250, 1041, 5301, 47, 2, 198, 1, 520, 9531, 198, 71, 1059, 38668, 216, 33, 34, 31, 38, 32, 446, 1885, 33691, 33, 34, 31, 38, 32, 45, 32, 30, 34, 7791, 32, 30, 34, 567, 8427, 30, 198, 23830, 216, 37, 32, 3487, 28, 1041, 11420, 216, 32, 30, 34, 1792, 216, 37, 32, 446, 1885, 33691, 32, 30, 34, 26, 37, 32, 45, 33, 32, 7791, 33, 32, 30, 198, 1229, 216, 33, 32, 2, 198]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWeng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?<|im_end|>\\n<|im_start|>assistant\\nWeng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10<|im_end|>\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_dataset(sample):\n",
    "    # TODO: 🐕 Convert the sample into a chat format\n",
    "    # 1. create a message format with the role and content\n",
    "    user_text = sample['question']\n",
    "    assistant_text = sample['answer']\n",
    "    # 2. apply the chat template to the samples using the tokenizer's method\n",
    "    sample['messages'] = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "        ],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "\n",
    "ds_processed = ds.map(process_dataset)\n",
    "print(ds_processed)\n",
    "print(\"----------------------------\")\n",
    "print(ds_processed['train'][1])\n",
    "tokenizer.decode(ds_processed['train'][1]['messages'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
